{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "See [guide](https://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b8cc06f648e23a2"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-03T17:22:20.880846800Z",
     "start_time": "2024-04-03T17:22:20.347586500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from AudioUtil import AudioUtil as au\n",
    "import torch\n",
    "import torchaudio\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: fender-jakob. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.5"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>C:\\code\\dspro2\\wandb\\run-20240403_192222-0vuk79y1</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/fender-jakob/Preliminary%20Baseline/runs/0vuk79y1/workspace' target=\"_blank\">lemon-grass-1</a></strong> to <a href='https://wandb.ai/fender-jakob/Preliminary%20Baseline' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/fender-jakob/Preliminary%20Baseline' target=\"_blank\">https://wandb.ai/fender-jakob/Preliminary%20Baseline</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/fender-jakob/Preliminary%20Baseline/runs/0vuk79y1/workspace' target=\"_blank\">https://wandb.ai/fender-jakob/Preliminary%20Baseline/runs/0vuk79y1/workspace</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fender-jakob/Preliminary%20Baseline/runs/0vuk79y1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x29d73b0e680>"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"Preliminary Baseline\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T17:22:23.145889Z",
     "start_time": "2024-04-03T17:22:20.881871800Z"
    }
   },
   "id": "bbfb1d989059eb6b",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   rating playback_used ebird_code    channels        date          pitch  \\\n0     3.5            no     aldfly    1 (mono)  2013-05-25  Not specified   \n1     4.0            no     aldfly  2 (stereo)  2013-05-27           both   \n2     4.0            no     aldfly  2 (stereo)  2013-05-27           both   \n3     3.5            no     aldfly  2 (stereo)  2013-05-27           both   \n4     4.0            no     aldfly  2 (stereo)  2013-05-27           both   \n\n   duration      filename          speed           species  ...   xc_id  \\\n0        25  XC134874.mp3  Not specified  Alder Flycatcher  ...  134874   \n1        36  XC135454.mp3           both  Alder Flycatcher  ...  135454   \n2        39  XC135455.mp3           both  Alder Flycatcher  ...  135455   \n3        33  XC135456.mp3           both  Alder Flycatcher  ...  135456   \n4        36  XC135457.mp3          level  Alder Flycatcher  ...  135457   \n\n                                 url        country            author  \\\n0  https://www.xeno-canto.org/134874  United States  Jonathon Jongsma   \n1  https://www.xeno-canto.org/135454  United States       Mike Nelson   \n2  https://www.xeno-canto.org/135455  United States       Mike Nelson   \n3  https://www.xeno-canto.org/135456  United States       Mike Nelson   \n4  https://www.xeno-canto.org/135457  United States       Mike Nelson   \n\n                        primary_label longitude         length   time  \\\n0  Empidonax alnorum_Alder Flycatcher   -92.962  Not specified   8:00   \n1  Empidonax alnorum_Alder Flycatcher  -82.1106         0-3(s)  08:30   \n2  Empidonax alnorum_Alder Flycatcher  -82.1106         0-3(s)  08:30   \n3  Empidonax alnorum_Alder Flycatcher  -82.1106         0-3(s)  08:30   \n4  Empidonax alnorum_Alder Flycatcher  -82.1106         0-3(s)  08:30   \n\n          recordist                                            license  \n0  Jonathon Jongsma        Creative Commons Attribution-ShareAlike 3.0  \n1       Mike Nelson  Creative Commons Attribution-NonCommercial-Sha...  \n2       Mike Nelson  Creative Commons Attribution-NonCommercial-Sha...  \n3       Mike Nelson  Creative Commons Attribution-NonCommercial-Sha...  \n4       Mike Nelson  Creative Commons Attribution-NonCommercial-Sha...  \n\n[5 rows x 35 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rating</th>\n      <th>playback_used</th>\n      <th>ebird_code</th>\n      <th>channels</th>\n      <th>date</th>\n      <th>pitch</th>\n      <th>duration</th>\n      <th>filename</th>\n      <th>speed</th>\n      <th>species</th>\n      <th>...</th>\n      <th>xc_id</th>\n      <th>url</th>\n      <th>country</th>\n      <th>author</th>\n      <th>primary_label</th>\n      <th>longitude</th>\n      <th>length</th>\n      <th>time</th>\n      <th>recordist</th>\n      <th>license</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.5</td>\n      <td>no</td>\n      <td>aldfly</td>\n      <td>1 (mono)</td>\n      <td>2013-05-25</td>\n      <td>Not specified</td>\n      <td>25</td>\n      <td>XC134874.mp3</td>\n      <td>Not specified</td>\n      <td>Alder Flycatcher</td>\n      <td>...</td>\n      <td>134874</td>\n      <td>https://www.xeno-canto.org/134874</td>\n      <td>United States</td>\n      <td>Jonathon Jongsma</td>\n      <td>Empidonax alnorum_Alder Flycatcher</td>\n      <td>-92.962</td>\n      <td>Not specified</td>\n      <td>8:00</td>\n      <td>Jonathon Jongsma</td>\n      <td>Creative Commons Attribution-ShareAlike 3.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.0</td>\n      <td>no</td>\n      <td>aldfly</td>\n      <td>2 (stereo)</td>\n      <td>2013-05-27</td>\n      <td>both</td>\n      <td>36</td>\n      <td>XC135454.mp3</td>\n      <td>both</td>\n      <td>Alder Flycatcher</td>\n      <td>...</td>\n      <td>135454</td>\n      <td>https://www.xeno-canto.org/135454</td>\n      <td>United States</td>\n      <td>Mike Nelson</td>\n      <td>Empidonax alnorum_Alder Flycatcher</td>\n      <td>-82.1106</td>\n      <td>0-3(s)</td>\n      <td>08:30</td>\n      <td>Mike Nelson</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.0</td>\n      <td>no</td>\n      <td>aldfly</td>\n      <td>2 (stereo)</td>\n      <td>2013-05-27</td>\n      <td>both</td>\n      <td>39</td>\n      <td>XC135455.mp3</td>\n      <td>both</td>\n      <td>Alder Flycatcher</td>\n      <td>...</td>\n      <td>135455</td>\n      <td>https://www.xeno-canto.org/135455</td>\n      <td>United States</td>\n      <td>Mike Nelson</td>\n      <td>Empidonax alnorum_Alder Flycatcher</td>\n      <td>-82.1106</td>\n      <td>0-3(s)</td>\n      <td>08:30</td>\n      <td>Mike Nelson</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.5</td>\n      <td>no</td>\n      <td>aldfly</td>\n      <td>2 (stereo)</td>\n      <td>2013-05-27</td>\n      <td>both</td>\n      <td>33</td>\n      <td>XC135456.mp3</td>\n      <td>both</td>\n      <td>Alder Flycatcher</td>\n      <td>...</td>\n      <td>135456</td>\n      <td>https://www.xeno-canto.org/135456</td>\n      <td>United States</td>\n      <td>Mike Nelson</td>\n      <td>Empidonax alnorum_Alder Flycatcher</td>\n      <td>-82.1106</td>\n      <td>0-3(s)</td>\n      <td>08:30</td>\n      <td>Mike Nelson</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.0</td>\n      <td>no</td>\n      <td>aldfly</td>\n      <td>2 (stereo)</td>\n      <td>2013-05-27</td>\n      <td>both</td>\n      <td>36</td>\n      <td>XC135457.mp3</td>\n      <td>level</td>\n      <td>Alder Flycatcher</td>\n      <td>...</td>\n      <td>135457</td>\n      <td>https://www.xeno-canto.org/135457</td>\n      <td>United States</td>\n      <td>Mike Nelson</td>\n      <td>Empidonax alnorum_Alder Flycatcher</td>\n      <td>-82.1106</td>\n      <td>0-3(s)</td>\n      <td>08:30</td>\n      <td>Mike Nelson</td>\n      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 35 columns</p>\n</div>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only use aldfly and bkcchi for example purposes\n",
    "df = pd.read_csv('input/birdsong-recognition/train.csv')\n",
    "\n",
    "df = df[df['ebird_code'].isin(['aldfly', 'bkcchi'])]\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T17:22:23.333398800Z",
     "start_time": "2024-04-03T17:22:23.146890400Z"
    }
   },
   "id": "1bcc341f63373622",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from AudioUtil import AudioUtil\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio\n",
    "\n",
    "class_ids_dict = {'aldfly': 0, 'bkcchi': 1}\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Sound Dataset\n",
    "# ----------------------------\n",
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df, data_path):\n",
    "        self.df = df\n",
    "        self.data_path = str(data_path)\n",
    "        self.duration = 4000\n",
    "        self.sr = 44100\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.4\n",
    "\n",
    "    # ----------------------------\n",
    "    # Number of items in dataset\n",
    "    # ----------------------------\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "        # ----------------------------\n",
    "\n",
    "    # Get i'th item in dataset\n",
    "    # ----------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        # Absolute file path of the audio file - concatenate the audio directory with\n",
    "        # the relative path\n",
    "        audio_file = self.data_path + self.df.loc[idx, 'ebird_code'] + '/' + self.df.loc[idx, 'filename']\n",
    "        # Get the Class ID\n",
    "        ebird_code = self.df.loc[idx, 'ebird_code']\n",
    "        class_id = class_ids_dict[ebird_code]\n",
    "\n",
    "        aud = AudioUtil.open(audio_file)\n",
    "        # Some sounds have a higher sample rate, or fewer channels compared to the\n",
    "        # majority. So make all sounds have the same number of channels and same \n",
    "        # sample rate. Unless the sample rate is the same, the pad_trunc will still\n",
    "        # result in arrays of different lengths, even though the sound duration is\n",
    "        # the same.\n",
    "        reaud = AudioUtil.resample(aud, self.sr)\n",
    "        rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "\n",
    "        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "        sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "        return aug_sgram, class_id"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T17:22:23.357003400Z",
     "start_time": "2024-04-03T17:22:23.336398900Z"
    }
   },
   "id": "130ed5bcc13fe40e",
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "data_path = \"./input/birdsong-recognition/train_audio/\"\n",
    "\n",
    "myds = SoundDS(df, data_path)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T17:22:23.394820700Z",
     "start_time": "2024-04-03T17:22:23.349936800Z"
    }
   },
   "id": "2b8e311f1935ca7b",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Audio Classification Model\n",
    "# ----------------------------\n",
    "class AudioClassifier(nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(2, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=10)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    "\n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel = AudioClassifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T17:22:23.419311800Z",
     "start_time": "2024-04-03T17:22:23.367782900Z"
    }
   },
   "id": "f161940dc0078a10",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.24, Accuracy: 0.19\n",
      "Epoch: 1, Loss: 2.19, Accuracy: 0.44\n",
      "Epoch: 2, Loss: 2.12, Accuracy: 0.60\n",
      "Epoch: 3, Loss: 2.01, Accuracy: 0.62\n",
      "Epoch: 4, Loss: 1.97, Accuracy: 0.61\n",
      "Epoch: 5, Loss: 1.92, Accuracy: 0.66\n",
      "Epoch: 6, Loss: 1.83, Accuracy: 0.66\n",
      "Epoch: 7, Loss: 1.80, Accuracy: 0.65\n",
      "Epoch: 8, Loss: 1.77, Accuracy: 0.64\n",
      "Epoch: 9, Loss: 1.77, Accuracy: 0.63\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "def training(model, train_dl, num_epochs):\n",
    "    # Loss Function, Optimizer and Scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                    steps_per_epoch=int(len(train_dl)),\n",
    "                                                    epochs=num_epochs,\n",
    "                                                    anneal_strategy='linear')\n",
    "\n",
    "    # Repeat for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "\n",
    "        # Repeat for each batch in the training set\n",
    "        for i, data in enumerate(train_dl):\n",
    "            # Get the input features and target labels, and put them on the GPU\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Normalize the inputs\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Keep stats for Loss and Accuracy\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Get the predicted class with the highest score\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            # Count of predictions that matched the target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "\n",
    "            #if i % 10 == 0:    # print every 10 mini-batches\n",
    "            #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "\n",
    "        # Print stats at the end of the epoch\n",
    "        num_batches = len(train_dl)\n",
    "        avg_loss = running_loss / num_batches\n",
    "        acc = correct_prediction / total_prediction\n",
    "        wandb.log({\"Epoch\": epoch, \"Loss\": avg_loss, \"Accuracy\": acc})\n",
    "        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "num_epochs = 10  # Just for demo, adjust this higher.\n",
    "training(myModel, train_dl, num_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T17:23:40.689871200Z",
     "start_time": "2024-04-03T17:22:23.378611100Z"
    }
   },
   "id": "ac4bf00789bdfbdb",
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72, Total items: 40\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Inference\n",
    "# ----------------------------\n",
    "def inference (model, val_dl):\n",
    "  correct_prediction = 0\n",
    "  total_prediction = 0\n",
    "\n",
    "  # Disable gradient updates\n",
    "  with torch.no_grad():\n",
    "    for data in val_dl:\n",
    "      # Get the input features and target labels, and put them on the GPU\n",
    "      inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "      # Normalize the inputs\n",
    "      inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "      inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "      # Get predictions\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      # Get the predicted class with the highest score\n",
    "      _, prediction = torch.max(outputs,1)\n",
    "      # Count of predictions that matched the target label\n",
    "      correct_prediction += (prediction == labels).sum().item()\n",
    "      total_prediction += prediction.shape[0]\n",
    "    \n",
    "  acc = correct_prediction/total_prediction\n",
    "  wandb.log({\"Validation Accuracy\": acc})\n",
    "  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
    "\n",
    "# Run inference on trained model with the validation set\n",
    "inference(myModel, val_dl)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T17:23:49.094671500Z",
     "start_time": "2024-04-03T17:23:46.964931600Z"
    }
   },
   "id": "922c5661ba999e9c",
   "execution_count": 63
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
